import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

pd.set_option("display.max_columns", None)
plt.style.use('seaborn-v0_8')

# Load cleaned dataset
df = pd.read_csv("../data_processed/cleaned_customers.csv")

df.head()
#IMPORT AND LOAD DATA
df.info()
print("\nMissing values:\n", df.isna().sum())
print("\nDuplicate rows:", df.duplicated().sum())
df.describe()
#BASIC STRUCTURE CHECK

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Select only numerical columns (exclude categorical and IDs)
num_cols = df.select_dtypes(include=np.number).columns.tolist()
num_cols = [col for col in num_cols if col != 'Customer_ID']

# Function to calculate outlier percentage using IQR
def outlier_percent(data):
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    num_outliers = np.sum((data < lower) | (data > upper))
    total = data.count()
    return round((num_outliers / total) * 100, 2)

# Display outlier percentage for each numeric column
print("üîπ Outlier Percentage by Feature:\n")
for col in num_cols:
    percent = outlier_percent(df[col])
    print(f'Outliers in "{col}": {percent}%')

# Visualize outliers with boxplots 
plt.figure(figsize=(14, 8))
df[num_cols].boxplot(rot=45, patch_artist=True,
                     boxprops=dict(facecolor="lightblue", color="blue"),
                     medianprops=dict(color="red"))
plt.title("Boxplot Visualization of Outliers")
plt.ylabel("Value Range")
plt.show()

cat_cols = ["Gender","Education","MaritalStatus","Income","CardType"]

for col in cat_cols:
    print(f"\nüîπ {col} distribution:")
    print(df[col].value_counts())
    sns.countplot(data=df, x=col, palette="coolwarm")
    plt.title(f"{col} Distribution")
    plt.xticks(rotation=45)
    plt.show()
    #EXPLORE CATEGORICAL FEATURE

import matplotlib.pyplot as plt
import seaborn as sns

# Select only key numeric columns to plot
num_cols = [
    "Credit_Limit", 
    "Total_Revolving_Bal", 
    "Avg_Open_To_Buy",
    "Total_Amt_Chng_Q4_Q1",
    "TotalTransactionAmount",
    "TotalTransactionCount",
    "AvgUtilization",
    "Dependents"
]

# Create combined vertical plots
plt.figure(figsize=(10, len(num_cols) * 3))
for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols), 1, i)
    sns.histplot(df[col], kde=True, color="green", bins=30)
    plt.title(f"{col} Distribution", fontsize=11, fontweight='bold')
    plt.xlabel(col)
    plt.ylabel("Density")
    plt.tight_layout(pad=2)

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

cat_cols = ["Gender", "Education", "MaritalStatus", "Income", "CardType"]

plt.figure(figsize=(10, len(cat_cols)*2.5))
for i, col in enumerate(cat_cols, 1):
    plt.subplot(len(cat_cols), 1, i)
    ax = sns.countplot(data=df, x=col, palette="crest")
    plt.title(f"{col} Distribution", fontsize=11, fontweight='bold')
    plt.xlabel(col)
    plt.ylabel("Count")

    # Add percentage labels above bars
    total = len(df[col])
    for p in ax.patches:
        percentage = f'{100 * p.get_height() / total:.1f}%'
        ax.annotate(percentage, (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center', fontsize=9, color='black', xytext=(0, 8),
                    textcoords='offset points')

    plt.tight_layout(pad=2)

plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Define categorical columns
cat_cols = ["Gender", "Education", "MaritalStatus", "Income", "CardType"]

# Create one figure with multiple vertical subplots
plt.figure(figsize=(10, len(cat_cols) * 3))
for i, col in enumerate(cat_cols, 1):
    plt.subplot(len(cat_cols), 1, i)
    sns.countplot(data=df, x=col, palette="coolwarm", edgecolor="black")
    plt.title(f"{col} Distribution", fontsize=11, fontweight='bold')
    plt.xlabel("")
    plt.ylabel("Count")
    plt.xticks(rotation=45)
    plt.tight_layout(pad=2)

plt.show()

num_cols = [
    "Age", "Dependents", "Tenure", "RelationshipCount",
    "InactiveMonths", "ContactsLast12M", "Credit_Limit",
    "Total_Revolving_Bal", "TotalTransactionAmount",
    "TotalTransactionCount", "TransactionChangeRatio",
    "AvgUtilization"
]

df[num_cols].hist(bins=20, figsize=(15,10), color="#a0b9f0")
plt.suptitle("Histogram reprensentation of Numeric Feature", fontsize=16)
plt.show()
#EXPLORE NUMERIC FEATURE
# INSPECT COLUMN AND DTYPES
print("Columns:\n", df.columns.tolist(), "\n")
print("Data types:\n", df.dtypes, "\n")
# preview the likely columns
cols = ['TotalTransactionCount','TotalTransactionAmount','Total_Trans_Ct','Total_Trans_Amt',
        'Income','Income_Category','Credit_Limit']
for c in cols:
    if c in df.columns:
        print(f"Sample values for {c} ->")
        display(df[[c]].head(5))

# Convert numeric fields to numbers (coerce errors to NaN)
num_cols = ['TotalTransactionCount','TotalTransactionAmount','Credit_Limit']
for col in num_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

# Show counts of NaNs
print("NaNs per important column:")
print(df[num_cols].isna().sum())


import plotly.express as px

# choose actual names that exist
x_col = 'TotalTransactionCount'   # we normalized above
y_col = 'TotalTransactionAmount'
size_col = 'Credit_Limit'
color_col = 'Income'              # categorical

# confirm they exist
missing = [c for c in [x_col,y_col,size_col,color_col] if c not in df.columns]
if missing:
    raise ValueError(f"Missing columns for plot: {missing} ‚Äî check your column names from the inspect step.")

fig = px.scatter(
    df,
    x=x_col,
    y=y_col,
    color=color_col if color_col in df.columns else None,
    size=size_col if size_col in df.columns else None,
    title="Transaction Count vs Amount (colored by Income)",
    hover_data=[c for c in ['Age','CardType','AvgUtilization'] if c in df.columns]
)

# If Plotly doesn't render in VS Code, open in browser:
try:
    fig.show()
except Exception as e:
    print("Plotly renderer failed in notebook, opening in browser instead:", e)
    import plotly.io as pio
    pio.renderers.default = "browser"
    fig.show()
# PLOTY

# normalize column names to a safe standard (run this once)
df.columns = [c.strip() for c in df.columns]
rename_map = {}

# map common Kaggle names to our standard
if 'Total_Trans_Ct' in df.columns: rename_map['Total_Trans_Ct'] = 'TotalTransactionCount'
if 'Total_Trans_Amt' in df.columns: rename_map['Total_Trans_Amt'] = 'TotalTransactionAmount'
if 'TotalTransactionCount' in df.columns: rename_map['TotalTransactionCount'] = 'TotalTransactionCount'
if 'TotalTransactionAmount' in df.columns: rename_map['TotalTransactionAmount'] = 'TotalTransactionAmount'
if 'Income_Category' in df.columns: rename_map['Income_Category'] = 'Income'
if 'Income' in df.columns: rename_map['Income'] = 'Income'
if 'Credit_Limit' in df.columns: rename_map['Credit_Limit'] = 'Credit_Limit'

df = df.rename(columns=rename_map)
print("Renamed columns :", rename_map)

#credit limit vs utilization
sns.scatterplot(data=df, x="Credit_Limit", y="AvgUtilization", hue="Income")
plt.title("Credit Limit vs Average Utilization by Income")
plt.show()

#correlation heatmap
plt.figure(figsize=(20,8))
sns.heatmap(df.corr(numeric_only=True), cmap="coolwarm", annot=True)
plt.title("Correlation Heatmap")
plt.show()

# üß© FEATURE ENGINEERING

# Creating new derived features to capture customer spending, credit, and activity behavior

df["new_CreditUsage_Ratio"] = df["Total_Revolving_Bal"] / (df["Credit_Limit"] + 1)
df["new_Revolving_to_Available"] = df["Total_Revolving_Bal"] / (df["AvgUtilization"] + 1)

df["new_Avg_Transaction_Value"] = df["TotalTransactionAmount"] / (df["TotalTransactionCount"] + 1)
df["new_Transaction_Efficiency"] = df["TotalTransactionCount"] / (df["Tenure"] + 1)
df["new_Activity_Level"] = df["TotalTransactionAmount"] / (df["Tenure"] + 1)

df["new_Inactive_to_Tenure"] = df["InactiveMonths"] / (df["Tenure"] + 1)
df["new_Contacts_per_Tenure"] = df["ContactsLast12M"] / (df["Tenure"] + 1)
df["new_Dependents_to_Relationship"] = df["Dependents"] / (df["RelationshipCount"] + 1)

df["new_TransactionChange_Effect"] = df["TransactionChangeRatio"] * df["TotalTransactionCount"]
df["new_Spending_Change"] = df["TransactionChangeRatio"] * df["TotalTransactionAmount"]

df["new_Credit_to_Transaction"] = df["Credit_Limit"] / (df["TotalTransactionAmount"] + 1)
df["new_Balance_to_Transaction"] = df["Total_Revolving_Bal"] / (df["TotalTransactionAmount"] + 1)

# üîç Checking Missing Values
df.isnull().sum().sort_values(ascending=False).head()

# üßæ Setting Index (if Customer_ID present)
if "Customer_ID" in df.columns:
    df.set_index("Customer_ID", inplace=True)

df.head()

# ================================
# üìä CLUSTERING TECHNIQUE
# ================================

# Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Select relevant numeric columns for clustering
num_cols = [
    "Age", "Dependents", "Tenure", "RelationshipCount", "InactiveMonths",
    "ContactsLast12M", "Credit_Limit", "Total_Revolving_Bal",
    "TotalTransactionAmount", "TotalTransactionCount",
    "TransactionChangeRatio", "AvgUtilization"
]

# Filter only numeric features from the dataframe
X = df[num_cols].copy()

# -------------------------------
# STEP 1Ô∏è‚É£: DATA SCALING
# -------------------------------
# Standardize data to make all features equally important
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("‚úÖ Data scaling completed successfully!")

# -------------------------------
# STEP 2Ô∏è‚É£: ELBOW METHOD
# -------------------------------
# Helps find the optimal number of clusters (K)
inertia = []
K_range = range(2, 10)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

# Plot Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(K_range, inertia, 'o-', color="#6a5acd")
plt.title("Elbow Method to Determine Optimal K", fontsize=13)
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia / Within-Cluster Sum of Squares")
plt.grid(True)
plt.show()

# -------------------------------
# STEP 3Ô∏è‚É£: FIT K-MEANS MODEL
# -------------------------------
# Based on Elbow curve or domain understanding, choose K (example: 3)
kmeans_final = KMeans(n_clusters=3, random_state=42)
df["Cluster"] = kmeans_final.fit_predict(X_scaled)

print("‚úÖ K-Means clustering completed!")
print(df["Cluster"].value_counts())

# -------------------------------
# STEP 4Ô∏è‚É£: CLUSTER VALIDATION
# -------------------------------
# Measure clustering performance using silhouette score
score = silhouette_score(X_scaled, df["Cluster"])
print(f"‚ú® Silhouette Score: {score:.3f}")

# -------------------------------
# STEP 5Ô∏è‚É£: VISUALIZE CLUSTERS
# -------------------------------
# Visualizing with 2 variables for simplicity
plt.figure(figsize=(8, 6))
sns.scatterplot(
    x=X["TotalTransactionAmount"],
    y=X["Credit_Limit"],
    hue=df["Cluster"],
    palette="viridis",
    s=70
)
plt.title("Customer Segmentation Based on Transactions and Credit Limit", fontsize=13)
plt.xlabel("Total Transaction Amount")
plt.ylabel("Credit Limit")
plt.legend(title="Cluster")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage

# ===============================
# 1Ô∏è‚É£ Data Preparation
# ===============================

# Assuming your cleaned dataset is loaded as df
# Replace with your dataframe variable if different
data = df.copy()

# Select only numeric columns for clustering
num_cols = data.select_dtypes(include=[np.number]).columns
X = data[num_cols].dropna()

# Standardize data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# ===============================
# 2Ô∏è‚É£ DENDROGRAM
# ===============================
plt.figure(figsize=(12, 6))
plt.title("Dendrogram for Hierarchical Clustering", fontsize=14, fontweight='bold')
plt.xlabel("Data Points")
plt.ylabel("Euclidean Distance")

# Linkage method = 'ward' (minimizes variance within clusters)
Z = linkage(X_scaled, method='ward')

dendrogram(Z, truncate_mode="level", p=5, color_threshold=0.7 * np.max(Z[:, 2]))
plt.show()


silhouette_scores = []
K = range(2, 8)  # Testing cluster counts between 2‚Äì7
for k in K:
    model = AgglomerativeClustering(n_clusters=k, linkage='ward')
    labels = model.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    silhouette_scores.append(score)

# Plot Silhouette Scores
plt.figure(figsize=(8, 4))
plt.plot(K, silhouette_scores, marker='o', color="#6a5acd")
plt.title("Silhouette Score for Different Cluster Counts", fontsize=13, fontweight='bold')
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.show()

# ===============================
# 4Ô∏è‚É£ AGGLOMERATIVE CLUSTERING (Using 3 clusters as example)
# ===============================
agg_model = AgglomerativeClustering(n_clusters=3, linkage='ward')
cluster_labels = agg_model.fit_predict(X_scaled)

# Add cluster labels to original data
data['Cluster'] = cluster_labels

# ===============================
# 5Ô∏è‚É£ CLUSTER VISUALIZATION
# ===============================
plt.figure(figsize=(7,5))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=data['Cluster'], palette='viridis')
plt.title("Agglomerative Clustering (3 Clusters)", fontsize=13, fontweight='bold')
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.legend(title='Cluster')
plt.show()


# -----------------------------
# Hierarchical Clustering + PCA + Silhouette + Agglomerative
# Ready-to-run block for your Financial Segmentation project
# -----------------------------

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage

# -----------------------------
# 1) Choose numeric features (adjust list if your dataset uses different names)
# -----------------------------
num_cols = [
    "Age", "Dependents", "Tenure", "RelationshipCount", "InactiveMonths",
    "ContactsLast12M", "Credit_Limit", "Total_Revolving_Bal",
    "TotalTransactionAmount", "TotalTransactionCount",
    "TransactionChangeRatio", "AvgUtilization"
]

# Keep only those columns present in your df
num_cols = [c for c in num_cols if c in df.columns]

# Basic check
if len(num_cols) == 0:
    raise ValueError("No numeric columns found from the predefined list. Update `num_cols` to match your df.")

# -----------------------------
# 2) Prepare numeric dataframe (impute median for missing values)
# -----------------------------
X_num = df[num_cols].copy()
X_num = X_num.fillna(X_num.median())    # simple median imputation

# Optionally: check for any remaining NaNs
if X_num.isna().any().any():
    print("Warning: NaNs remain after imputation. Rows with NaNs will be dropped for clustering.")
    X_num = X_num.dropna()

# -----------------------------
# 3) Scale the data
# -----------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_num)

# -----------------------------
# 4) PCA for visualization (2 components)
# -----------------------------
pca = PCA(n_components=2, random_state=42)
X_principal = pca.fit_transform(X_scaled)

print("PCA explained variance ratio (2 components):", np.round(pca.explained_variance_ratio_, 3))
print("Total explained variance (2 comps):", np.round(pca.explained_variance_ratio_.sum(), 3))

# -----------------------------
# A) DENDROGRAM (use PCA-reduced data for clearer structure)
# -----------------------------
plt.figure(figsize=(10, 6))
plt.title('Dendrogram (Ward linkage) - PCA(2) projection')
Z = linkage(X_principal, method='ward')
dendrogram(Z, truncate_mode='level', p=5, color_threshold=None)
plt.xlabel('Sample index (or cluster)')
plt.ylabel('Distance (Ward)')
plt.show()

# -----------------------------
# B) SILHOUETTE SCORE for different cluster counts (2..7)
# -----------------------------
silhouette_scores = []
cluster_range = range(2, 8)
for k in cluster_range:
    labels = AgglomerativeClustering(n_clusters=k, linkage='ward').fit_predict(X_principal)
    score = silhouette_score(X_principal, labels)
    silhouette_scores.append(score)

plt.figure(figsize=(8,4))
plt.plot(list(cluster_range), silhouette_scores, marker='o', color='#6a5acd')
plt.xticks(list(cluster_range))
plt.xlabel("Number of clusters (k)")
plt.ylabel("Silhouette score")
plt.title("Silhouette Score (Agglomerative on PCA(2))")
plt.grid(True)
plt.show()

# Print silhouette values
for k, s in zip(cluster_range, silhouette_scores):
    print(f"k = {k} ‚Üí silhouette = {s:.4f}")

# -----------------------------
# C) AGGLOMERATIVE CLUSTERING (choose n_clusters per dendrogram / silhouette)
#    User requested n_clusters = 3 for hierarchical example
# -----------------------------
n_clusters_chosen = 3   # change to 5 if you want finer segmentation for PCA visualization

agg = AgglomerativeClustering(n_clusters=n_clusters_chosen, linkage='ward')
cluster_labels = agg.fit_predict(X_principal)

# Attach labels to a copy of your original data aligned with X_principal rows
result_df = X_num.reset_index(drop=True).copy()
result_df['PC1'] = X_principal[:,0]
result_df['PC2'] = X_principal[:,1]
result_df['Cluster'] = cluster_labels

# -----------------------------
# Visualization: PCA scatter colored by cluster
# -----------------------------
plt.figure(figsize=(8,6))
palette = sns.color_palette("tab10", n_clusters_chosen)
sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=result_df, palette=palette, s=40, alpha=0.9)
plt.title(f"Agglomerative Clustering (n_clusters={n_clusters_chosen}) on PCA(2) projection")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title='Cluster', loc='best')
plt.show()

# -----------------------------
# Cluster summary (means of numeric features per cluster)
# -----------------------------
summary = result_df.groupby('Cluster')[num_cols].mean().round(2)
counts = result_df['Cluster'].value_counts().sort_index()
print("\nCluster counts:\n", counts)
print("\nCluster summary (mean values):\n", summary)

# Optional: save cluster labels back to original df if indices align (be careful with rows dropped)
# df.loc[X_num.index, 'Cluster'] = cluster_labels

# -----------------------------
# B) SILHOUETTE SCORE (BAR GRAPH VERSION)
# -----------------------------
from sklearn.metrics import silhouette_score
from sklearn.cluster import AgglomerativeClustering
import matplotlib.pyplot as plt

silhouette_scores = []
cluster_range = range(2, 8)

for n_cluster in cluster_range:
    labels = AgglomerativeClustering(n_clusters=n_cluster, linkage='ward').fit_predict(X_principal)
    score = silhouette_score(X_principal, labels)
    silhouette_scores.append(score)

# Bar graph to visualize silhouette scores
plt.figure(figsize=(8, 5))
bars = plt.bar(cluster_range, silhouette_scores, color="#e67e22", edgecolor="black")

# Annotate bars with score values
for bar, score in zip(bars, silhouette_scores):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() - 0.02,
             f"{score:.2f}", ha='center', va='bottom', color='white', fontsize=10, fontweight='bold')

plt.xlabel("Number of Clusters (k)", fontsize=11)
plt.ylabel("Silhouette Score", fontsize=11)
plt.title("Silhouette Scores for Different Cluster Counts (Agglomerative)", fontsize=13, fontweight="bold")
plt.xticks(cluster_range)
plt.ylim(0, max(silhouette_scores) + 0.1)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.show()

# Print values in console too
for k, s in zip(cluster_range, silhouette_scores):
    print(f"Number of clusters = {k} ‚Üí Silhouette Score = {s:.4f}")
